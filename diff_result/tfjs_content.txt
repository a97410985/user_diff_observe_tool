diff --git a/a.js b/b.js
@@ -22,7 +22,7 @@
A blob:ae7b8cabd39afe4ffccfd712a210cfeda076d284
// ./scripts/tag-version.js DIR_NAME
// Where DIR_NAME is the directory name for the package you want to make a
// version for.
const fs = require('fs');

let dirName = process.argv[2];
if (dirName.endsWith('/')) {
  dirName = dirName.substr(0, dirName.length - 1);
}
const packageJsonFile = dirName + '/package.json';
if (!fs.existsSync(packageJsonFile)) {
  console.log(
diff --git a/a.js b/b.js
@@ -40,7 +40,7 @@
A blob:a9e146b1f111decc4777b0d11c536cfeb69a0c6b
// directory to work.
if (os.platform() !== 'win32') {
  throw new Exception('Dep staging is only supported on Windows');
}

// Some windows machines contain a trailing " char:
if (targetDir != undefined && targetDir.endsWith('"')) {
  targetDir = targetDir.substr(0, targetDir.length - 1);
}

// Setup dest binary paths:
const destLibTensorFlowPath = path.join(targetDir, destLibTensorFlowName);
diff --git a/a.js b/b.js
@@ -31,14 +31,7 @@ module.exports = function(env, argv) {
A blob:1f3591ca086edf85df707fc45e9186377a91c33c
 */

const path = require('path');
const TerserPlugin = require('terser-webpack-plugin');

module.exports = function(env, argv) {
  const outputPath = argv.useCustomTfjs ? 'dist/custom' : 'dist/full'

  const config = {
    mode: 'production',
    entry: './app.js',
    target: 'web',
    output: {
      path: path.resolve(__dirname, outputPath),
      filename: 'app_webpack.js',
    },
    optimization: {
      minimizer: [
        new TerserPlugin({
          cache: true,
          parallel: true,
          sourceMap: false,
          terserOptions: {
            comments: false,
          }
        }),
      ]
    },
    module: {
      rules: [
diff --git a/a.js b/b.js
@@ -6,7 +6,7 @@
A blob:144070c34cf1c265fd08dcf454edbb6de23a4178

module.exports = {
  mode: 'development',
  entry: {
    app: './src/index.js',
  },
  devServer: {
    contentBase: './dist',
  },
  plugins: [
    new HtmlWebpackPlugin({
      title: 'Webpack sample app',
diff --git a/a.js b/b.js
@@ -22,8 +22,7 @@
A blob:92b45eaff9c0e376f592a541994f9b10b17ba912
import * as tf from '@tensorflow/tfjs';
import {CLASSES} from './classes';
import imageURL from './image1.jpg';
import image2URL from './image2.jpg';

const GOOGLE_CLOUD_STORAGE_DIR =
    'https://storage.googleapis.com/tfjs-models/savedmodel/';
const MODEL_URL =
    GOOGLE_CLOUD_STORAGE_DIR + 'coco-ssd-mobilenet_v1/model.json';

let modelPromise;

window.onload = () => modelPromise = tf.loadGraphModel(MODEL_URL);
diff --git a/a.js b/b.js
@@ -48,33 +47,7 @@ button.onclick = () => {
A blob:92b45eaff9c0e376f592a541994f9b10b17ba912
let modelPromise;

window.onload = () => modelPromise = tf.loadGraphModel(MODEL_URL);

const button = document.getElementById('toggle');
button.onclick = () => {
  image.src = image.src.endsWith(imageURL) ? image2URL : imageURL;
};

const image = document.getElementById('image');
image.src = imageURL;

const runButton = document.getElementById('run');
runButton.onclick = async () => {
  const model = await modelPromise;
  const pixels = tf.browser.fromPixels(image);
  console.log('model loaded');
  console.time('predict1');
  const res1 = await model.executeAsync(pixels.reshape([1, ...pixels.shape]));
  res1.map(t => t.dataSync());
  console.timeEnd('predict1');
  console.time('predict2');
  const res2 = await model.executeAsync(pixels.reshape([1, ...pixels.shape]));
  const count = res2[3].dataSync()[0];
  const boxes = res2[0].dataSync();
  const scores = res2[1].dataSync();
  const classes = res2[2].dataSync();
  console.timeEnd('predict2');


  const c = document.getElementById('canvas');
  const context = c.getContext('2d');
  context.drawImage(image, 0, 0);
  context.font = '10px Arial';

  console.log('number of detections: ', count);
  for (let i = 0; i < count; i++) {
    const min_y = boxes[i * 4] * 399;
    const min_x = boxes[i * 4 + 1] * 600;
    const max_y = boxes[i * 4 + 2] * 399;
    const max_x = boxes[i * 4 + 3] * 600;

    context.beginPath();
    context.rect(min_x, min_y, max_x - min_x, max_y - min_y);
    context.lineWidth = 1;
    context.strokeStyle = 'black';
    context.stroke();
    context.fillText(
        scores[i].toFixed(3) + ' ' + CLASSES.find(label => label.id === classes[i]).display_name,
        min_x, min_y - 5);
  }
};


diff --git a/a.js b/b.js
@@ -277,13 +277,22 @@ async function timeInference(predict, numRuns = 1) {
A blob:94d375d9e735d340c28f741453a8ff77ef4a352f
 * ```
 *
 * @param predict The predict function to execute and time.
 * @param numRuns The number of rounds for `predict` to execute and time.
 */
async function timeInference(predict, numRuns = 1) {
  if (typeof predict !== 'function') {
    throw new Error(
        'The first parameter should be a function, while ' +
        `a(n) ${typeof predict} is found.`);
  }

  const times = [];
  for (let i = 0; i < numRuns; i++) {
    const start = performance.now();
    const res = await predict();
    // The prediction can be tf.Tensor|tf.Tensor[]|{[name: string]: tf.Tensor}.
    const value = await downloadValuesFromTensorContainer(res);
    const elapsedTime = performance.now() - start;

    tf.dispose(res);
    times.push(elapsedTime);
  }

  const averageTime = times.reduce((acc, curr) => acc + curr, 0) / times.length;
  const minTime = Math.min(...times);
  const maxTime = Math.max(...times);
  const timeInfo = {
    times,
    averageTime,
    minTime,
    maxTime

  };
  return timeInfo;
}

/**
 * Downloads the values from the `tensorContainer` from any `tf.Tensor`s found
 * within the `tensorContainer`. Returns a promise of `TypedArray` or
 * `TypedArray[]` that resolves when the computation has finished.
 *
 * The values are asynchronously downloaded in parallel.
 *
 * @param tensorContainer The container of tensors to be downloaded.
 */
async function downloadValuesFromTensorContainer(tensorContainer) {
  let valueContainer;
  if (tensorContainer instanceof tf.Tensor) {
    valueContainer = await tensorContainer.data();
  } else if (Array.isArray(tensorContainer)) {
    // Start value downloads from all tensors.
    const valuePromiseContainer = tensorContainer.map(async item => {
      if (item instanceof tf.Tensor) {
        return item.data();
      }
      return item;
    });
    // Wait until all values are downloaded.
diff --git a/a.js b/b.js
@@ -294,7 +303,11 @@ async function downloadValuesFromTensorContainer(tensorContainer) {
A blob:94d375d9e735d340c28f741453a8ff77ef4a352f
 *
 * The values are asynchronously downloaded in parallel.
 *
 * @param tensorContainer The container of tensors to be downloaded.
 */
async function downloadValuesFromTensorContainer(tensorContainer) {
  let valueContainer;
  if (tensorContainer instanceof tf.Tensor) {
    valueContainer = await tensorContainer.data();
  } else if (Array.isArray(tensorContainer)) {
    // Start value downloads from all tensors.
    const valuePromiseContainer = tensorContainer.map(async item => {
      if (item instanceof tf.Tensor) {
        return item.data();
      }
      return item;
    });
    // Wait until all values are downloaded.
    valueContainer = await Promise.all(valuePromiseContainer);
  } else if (tensorContainer != null && typeof tensorContainer === 'object') {
    const valuePromiseContainer = [];
    // Start value downloads from all tensors.
    for (const property in tensorContainer) {
      if (tensorContainer[property] instanceof tf.Tensor) {
        valuePromiseContainer.push(tensorContainer[property].data());
      } else {
        valuePromiseContainer.push(tensorContainer[property]);
      }
    }
diff --git a/a.js b/b.js
@@ -16,6 +16,7 @@
A blob:817486335fffa3afee59f2e7da1253877cbbd4a4
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

const fs = require('fs');
const https = require('https');
const HttpsProxyAgent = require('https-proxy-agent');
const path = require('os').platform() === 'win32' ? require('path') :
                                                    require('path').win32;
diff --git a/a.js b/b.js
@@ -36,6 +37,8 @@
A blob:817486335fffa3afee59f2e7da1253877cbbd4a4
 * Downloads and unpacks a given tarball or zip file at a given path.
 * @param {string} uri The path of the compressed file to download and extract.
 * @param {string} destPath The destination path for the compressed content.
 * @param {Function} callback Handler for when downloading and extraction is
 *     complete.
 */
async function downloadAndUnpackResource(uri, destPath, callback) {
  // If HTTPS_PROXY, https_proxy, HTTP_PROXY, or http_proxy is set
  const proxy = process.env['HTTPS_PROXY'] || process.env['https_proxy'] ||
      process.env['HTTP_PROXY'] || process.env['http_proxy'] || '';

diff --git a/a.js b/b.js
@@ -44,13 +47,13 @@ async function downloadAndUnpackResource(uri, destPath, callback) {
A blob:817486335fffa3afee59f2e7da1253877cbbd4a4
 * @param {string} uri The path of the compressed file to download and extract.
 * @param {string} destPath The destination path for the compressed content.
 * @param {Function} callback Handler for when downloading and extraction is
 *     complete.
 */
async function downloadAndUnpackResource(uri, destPath, callback) {
  // If HTTPS_PROXY, https_proxy, HTTP_PROXY, or http_proxy is set
  const proxy = process.env['HTTPS_PROXY'] || process.env['https_proxy'] ||
      process.env['HTTP_PROXY'] || process.env['http_proxy'] || '';

  // Using object destructuring to construct the options object for the
  // http request.  the '...url.parse(targetUri)' part fills in the host,
  // path, protocol, etc from the targetUri and then we set the agent to the
  // default agent which is overridden a few lines down if there is a proxy
  const options = {...url.parse(uri), agent: https.globalAgent};

  if (proxy !== '') {
    options.agent = new HttpsProxyAgent(proxy);
  }

  const request = https.get(options, response => {
    const bar = new ProgressBar('[:bar] :rate/bps :percent :etas', {
      complete: '=',
      incomplete: ' ',
      width: 30,
diff --git a/a.js b/b.js
@@ -277,13 +277,22 @@ async function timeInference(predict, numRuns = 1) {
A blob:3ab588d3be2b34af8a3599c0aeb29143716e69c9
 * ```
 *
 * @param predict The predict function to execute and time.
 * @param numRuns The number of rounds for `predict` to execute and time.
 */
async function timeInference(predict, numRuns = 1) {
  if (typeof predict !== 'function') {
    throw new Error(
        'The first parameter should be a function, while ' +
        `a(n) ${typeof predict} is found.`);
  }

  const times = [];
  for (let i = 0; i < numRuns; i++) {
    const start = performance.now();
    const res = await predict();
    // The prediction can be tf.Tensor|tf.Tensor[]|{[name: string]: tf.Tensor}.
    const value = await downloadValuesFromTensorContainer(res);
    const elapsedTime = performance.now() - start;

    tf.dispose(res);
    times.push(elapsedTime);
  }

  const averageTime = times.reduce((acc, curr) => acc + curr, 0) / times.length;
  const minTime = Math.min(...times);
  const maxTime = Math.max(...times);
  const timeInfo = {
    times,
    averageTime,
    minTime,
    maxTime

  };
  return timeInfo;
}

/**
 * Downloads the values from the `tensorContainer` from any `tf.Tensor`s found
 * within the `tensorContainer`. Returns a promise of `TypedArray` or
 * `TypedArray[]` that resolves when the computation has finished.
 *
 * The values are asynchronously downloaded in parallel.
 *
 * @param tensorContainer The container of tensors to be downloaded.
 */
async function downloadValuesFromTensorContainer(tensorContainer) {
  let valueContainer;
  if (tensorContainer instanceof tf.Tensor) {
    valueContainer = await tensorContainer.data();
  } else if (Array.isArray(tensorContainer)) {
    // Start value downloads from all tensors.
    const valuePromiseContainer = tensorContainer.map(async item => {
      if (item instanceof tf.Tensor) {
        return item.data();
      }
      return item;
    });
    // Wait until all values are downloaded.
diff --git a/a.js b/b.js
@@ -294,7 +303,11 @@ async function downloadValuesFromTensorContainer(tensorContainer) {
A blob:3ab588d3be2b34af8a3599c0aeb29143716e69c9
 *
 * The values are asynchronously downloaded in parallel.
 *
 * @param tensorContainer The container of tensors to be downloaded.
 */
async function downloadValuesFromTensorContainer(tensorContainer) {
  let valueContainer;
  if (tensorContainer instanceof tf.Tensor) {
    valueContainer = await tensorContainer.data();
  } else if (Array.isArray(tensorContainer)) {
    // Start value downloads from all tensors.
    const valuePromiseContainer = tensorContainer.map(async item => {
      if (item instanceof tf.Tensor) {
        return item.data();
      }
      return item;
    });
    // Wait until all values are downloaded.
    valueContainer = await Promise.all(valuePromiseContainer);
  } else if (tensorContainer != null && typeof tensorContainer === 'object') {
    const valuePromiseContainer = [];
    // Start value downloads from all tensors.
    for (const property in tensorContainer) {
      if (tensorContainer[property] instanceof tf.Tensor) {
        valuePromiseContainer.push(tensorContainer[property].data());
      } else {
        valuePromiseContainer.push(tensorContainer[property]);
      }
    }
diff --git a/a.js b/b.js
@@ -336,11 +349,13 @@ async function downloadValuesFromTensorContainer(tensorContainer) {
A blob:3ab588d3be2b34af8a3599c0aeb29143716e69c9
 *
 * The values are asynchronously downloaded in parallel.
 *
 * @param tensorContainer The container of tensors to be downloaded.
 */
async function downloadValuesFromTensorContainer(tensorContainer) {
  let valueContainer;
  if (tensorContainer instanceof tf.Tensor) {
    valueContainer = await tensorContainer.data();
  } else if (Array.isArray(tensorContainer)) {
    // Start value downloads from all tensors.
    const valuePromiseContainer = tensorContainer.map(async item => {
      if (item instanceof tf.Tensor) {
        return item.data();
      }
      return item;
    });
    // Wait until all values are downloaded.
    valueContainer = await Promise.all(valuePromiseContainer);
  } else if (tensorContainer != null && typeof tensorContainer === 'object') {
    const valuePromiseContainer = [];
    // Start value downloads from all tensors.
    for (const property in tensorContainer) {
      if (tensorContainer[property] instanceof tf.Tensor) {
        valuePromiseContainer.push(tensorContainer[property].data());
      } else {
        valuePromiseContainer.push(tensorContainer[property]);
      }
    }
    // Wait until all values are downloaded.
    valueContainer = await Promise.all(valuePromiseContainer);
  }
  return valueContainer;
}

/**
 * Executes the predict function for `model` (`model.predict` for
 * tf.LayersModel and `model.executeAsync` for tf.GraphModel) and returns a
 * promise that resolves with information about the memory usage:
 * - `newBytes`: the number of new bytes allocated.
 * - `newTensors`: the number of new tensors created.
 * - `peakBytes`: the peak number of bytes allocated.
 * - `kernels`: an array of kernel information objects about their input and
 * output shapes, number of bytes used, number of new tensors created and kernel
 * time (ms). The array is sorted by `kernelTimeMs` field in non-ascending
 * order.
 * - `aggregatedKernels`: an array of aggregated kernel information objects with
 * `name` and `timeMs` fields. The array is sorted by `timeMs` field in
 * non-ascending order.
 *
 * ```js
 * const modelUrl =
 *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';
 * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});
 * const zeros = tf.zeros([1, 224, 224, 3]);
 * const profileInfo = await profileModelInference(model, zeros);
 *
 * console.log(`newBytes: ${profileInfo.newBytes}`);
 * console.log(`newTensors: ${profileInfo.newTensors}`);
 * console.log(`peakBytes: ${profileInfo.peakBytes}`);
 * ```
 *
 * @param model An instance of tf.GraphModel or tf.LayersModel for profiling
 *     memory usage in the inference process.
 * @param input The input tensor container for model inference.
 * @param isTflite Whether a TFLite model is being profiled or not.
 */
async function profileModelInference(model, input, isTflite = false) {
  const predict = isTflite ? () => tfliteModel.predict(input) :
                             getPredictFnForModel(model, input);
  return profileInference(predict, isTflite);
}

/**
 * Executes `predict()` and returns a promise that resolves with information
diff --git a/a.js b/b.js
@@ -372,8 +387,9 @@ async function profileModelInference(model, input, isTflite = false) {
A blob:3ab588d3be2b34af8a3599c0aeb29143716e69c9
 * @param model An instance of tf.GraphModel or tf.LayersModel for profiling
 *     memory usage in the inference process.
 * @param input The input tensor container for model inference.
 * @param isTflite Whether a TFLite model is being profiled or not.
 */
async function profileModelInference(model, input, isTflite = false) {
  const predict = isTflite ? () => tfliteModel.predict(input) :
                             getPredictFnForModel(model, input);
  return profileInference(predict, isTflite);
}

/**
 * Executes `predict()` and returns a promise that resolves with information
 * about the memory usage:
 * - `newBytes`: the number of new bytes allocated.
 * - `newTensors`: the number of new tensors created.
 * - `peakBytes`: the peak number of bytes allocated.
 * - `kernels`: an array of kernel information objects about their input and
 * output shapes, number of bytes used, number of new tensors created and kernel
 * time (ms). The array is sorted by `kernelTimeMs` field in non-ascending
 * order.
 * - `aggregatedKernels`: an array of aggregated kernel information objects with
 * `name` and `timeMs` fields. The array is sorted by `timeMs` field in
 * non-ascending order.
 *
 * ```js
 * const modelUrl =
 *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';
 * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});
 * const zeros = tf.zeros([1, 224, 224, 3]);
 * const profileInfo = await profileInference(() =>
 * model.predict(zeros));
 *
 * console.log(`newBytes: ${profileInfo.newBytes}`);
 * console.log(`newTensors: ${profileInfo.newTensors}`);
 * console.log(`peakBytes: ${profileInfo.peakBytes}`);
 * ```
 *
 * @param predict The predict function to execute for profiling memory usage.
 * @param isTflite Whether a TFLite model is being profiled or not.
 */
async function profileInference(predict, isTflite = false) {
  if (typeof predict !== 'function') {
    throw new Error(
        'The first parameter should be a function, while ' +
        `a(n) ${typeof predict} is found.`);
diff --git a/a.js b/b.js
@@ -381,24 +397,38 @@ async function profileInference(predict, isTflite = false) {
A blob:3ab588d3be2b34af8a3599c0aeb29143716e69c9
 * ```
 *
 * @param predict The predict function to execute for profiling memory usage.
 * @param isTflite Whether a TFLite model is being profiled or not.
 */
async function profileInference(predict, isTflite = false) {
  if (typeof predict !== 'function') {
    throw new Error(
        'The first parameter should be a function, while ' +
        `a(n) ${typeof predict} is found.`);
  }

  let kernelInfo = {};
  if (isTflite) {
    await predict();
    const profileItems = tfliteModel.getProfilingResults();
    kernelInfo.kernels = profileItems.map(item => {
      return {
        name: item.nodeType,
        kernelTimeMs: item.nodeExecMs,
        // TODO: Shapes are not supported yet.
        inputShapes: [],
        outputShapes: [],
      };
    });
  } else {
    kernelInfo = await tf.profile(async () => {
      const res = await predict();
      await downloadValuesFromTensorContainer(res);
      tf.dispose(res);
    });
  }
  kernelInfo.kernels =
      kernelInfo.kernels.sort((a, b) => b.kernelTimeMs - a.kernelTimeMs);
  kernelInfo.aggregatedKernels = aggregateKernelTime(kernelInfo.kernels);
